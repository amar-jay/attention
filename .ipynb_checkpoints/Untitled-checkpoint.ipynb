{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "733c621c-24fc-4fba-8fe8-6287e5bac83c",
   "metadata": {},
   "source": [
    "Trying to understand scaling laws by [chinchilla]() using [nanogpt (scaling_laws.ipynb)]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "c053b265-6472-49fe-be82-fbd37c431168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import functional as F\n",
    "import pandas as pd\n",
    "import scipy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79a7084f-9244-4d1d-964b-46518ef0d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_params(seq_len, vocab_size, d_model, num_heads, num_layers):\n",
    "    \"\"\" Given a GPT calculate total number of parameters \"\"\"\n",
    "    ffw_size = 4*d_model # Represent the intermediate layer size in MLP. in GPT the number of intermediate features is always 4*d_model.\n",
    "    # token and position embeddings\n",
    "    embeddings = d_model * vocab_size + d_model * seq_len\n",
    "    # transformer blocks\n",
    "    attention = 3*d_model**2 + 3*d_model # weights and biases\n",
    "    attproj = d_model**2 + d_model\n",
    "    ffw = d_model*(ffw_size) + ffw_size\n",
    "    ffwproj = ffw_size*d_model + d_model\n",
    "    layernorms = 2*2*d_model\n",
    "    # dense\n",
    "    ln_f = 2*d_model\n",
    "    dense = d_model*vocab_size # note: no bias here\n",
    "    # note: embeddings are not included in the param count!\n",
    "    total_params = num_layers*(attention + attproj + ffw + ffwproj + layernorms) + ln_f + dense\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2876f158-b602-4972-aafd-7b03a1c6eb10",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "\n",
    "A model's parameter size can simply be derived by running `.parameters()` to recieve the parameter count. However, intuitively let's understand, how the GPT parameters is calculated.\n",
    "\n",
    "`embedding = d_model * vocab_size + d_model * seq_len`\n",
    "embedding represents the total number of parameters used in the embedding layer. The embedding layer is made of two parts _token embedding_ and the _positional embedding_. The size of the weights used in the token embedding is `(number of input channels (d_model), vocab_size)`\n",
    "\n",
    "since \n",
    "```\n",
    "embedding layer    =  Train Data     @ Transpose(Weight)\n",
    "(B, T, vocab_size) = (B, vocab_size) @ (T, vocab_size)  \n",
    "```\n",
    "\n",
    "As you can see, after the training data is hot encoded it has a size of `B x vocab_size`. So based on this the Weight dimensions is `Size(vocab_size, T)`. where `T` represents the number of channels (aka. embedding output dimension). So the total number of parameters in the weight is `vocab_size x T`.\n",
    "\n",
    "Similarly in the positional embedding, the weight dimension is dictated by the training data.\n",
    "\n",
    "```\n",
    "embedding layer    =  Train Data  @ Transpose(Weight)\n",
    "(B, T, vocab_size) = (B, seq_len) @ (T, seq_len)  \n",
    "```\n",
    "\n",
    "Positional embedding is simply done by numbering the input within a fixed context window. This context window size is the `seq_len`(or sequence length).\n",
    "Based on this, the weight that embeds this data is of the `Size(seq_len, T)`. So making the total embedding of the positional embedding `seq_len x T`.\n",
    "\n",
    "The total number of parameters used in the embedding later is the sum of these two layers since the values are summed before passing through the next layer.\n",
    "\n",
    "```\n",
    "attention = 3*d_model**2 + 3*d_model\n",
    "```\n",
    "\n",
    "You've come across __Quadratic Scaling__ or __Squared attention__. Its the thought that when double the length of the input sequence, the computational cost associated with the attention mechanism increases by a factor of four because computational cost of attention in a transformer model scales quadratically with the sequence length.\n",
    "\n",
    "How does attention work?\n",
    "```python\n",
    "k = []\n",
    "for i in keys:\n",
    "    ki = []\n",
    "    for j in values:\n",
    "        for k in j:\n",
    "            ki[k] += j\n",
    "\n",
    "\n",
    "def attention(query, keys, values):\n",
    "    # Q, K, V  ->  Size(T, 3*C)\n",
    "    # Initialize attention weights\n",
    "    attention_weights = [] # (T, T)\n",
    "    \n",
    "    # Calculate attention scores\n",
    "    for key in keys:\n",
    "        score = dot_product(query, key) # (T, T)\n",
    "        attention_weights.append(score)\n",
    "    \n",
    "    # Normalize attention weights using softmax\n",
    "    attention_weights = softmax(attention_weights) # (T, T)\n",
    "    \n",
    "    # Calculate weighted sum of values\n",
    "    context_vector = [0] * len(values[0]) #(C,)\n",
    "    for i in range(len(values)):\n",
    "        for j in range(len(values[0])):\n",
    "            context_vector[j] += attention_weights[i] * values[i][j]\n",
    "    \n",
    "    return context_vector\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "73c1d26f-f77e-4a80-b6fe-46c21ba360bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, C = 1, 4, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "fb64b530-3d2d-4c13-8a3a-ba4e6c096430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[1., 2.],\n",
       "         [3., 4.],\n",
       "         [5., 6.],\n",
       "         [7., 8.]]]),\n",
       " array([[[1., 2.],\n",
       "         [2., 3.],\n",
       "         [3., 4.],\n",
       "         [4., 5.]]]))"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first implementation\n",
    "np.random.seed(10)\n",
    "x = np.linspace(1, 8, 8).reshape(B, T, C) \n",
    "# x /= B*T*C\n",
    "sol_1 = np.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        x_prev = x[b, :t+1] #(t, C)\n",
    "        sol_1[b, t] = np.mean(x_prev, 0)\n",
    "        \n",
    "\n",
    "x, sol_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "ac652bcd-c161-4859-9d59-8df67f5c31a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 2.0000],\n",
       "         [2.0000, 3.0000],\n",
       "         [3.0000, 4.0000],\n",
       "         [4.0000, 5.0000]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second implementation\n",
    "\n",
    "wei = torch.tril(torch.ones((T, T))) # ohh weight do not include the batch dimensions (B)\n",
    "print(wei,)\n",
    "wei = wei /wei.sum(1, keepdims=True)\n",
    "sol_2 = wei @ x\n",
    "\n",
    "# assert (sol_1 == sol_2).all()\n",
    "sol_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1fbbb63d-86e2-44bd-a55e-5b1d9f3fa860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 0. 0. 0.]\n",
      "  [1. 1. 0. 0.]\n",
      "  [1. 1. 1. 0.]\n",
      "  [1. 1. 1. 1.]]]\n",
      "[[[0.1 0.  0.  0. ]\n",
      "  [0.1 0.1 0.  0. ]\n",
      "  [0.1 0.1 0.1 0. ]\n",
      "  [0.1 0.1 0.1 0.1]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.00625, 0.0125 , 0.01875, 0.025  ],\n",
       "        [0.0375 , 0.05   , 0.0625 , 0.075  ],\n",
       "        [0.09375, 0.1125 , 0.13125, 0.15   ],\n",
       "        [0.175  , 0.2    , 0.225  , 0.25   ]]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# third implementation\n",
    "softmax = lambda v: np.exp(v) / np.sum(np.exp(v), )\n",
    "\n",
    "c = np.tril(np.ones((B, T, C)))\n",
    "print(c)\n",
    "masked_weight = np.ma.masked_array(c, mask=(c==0))\n",
    "filled_weight = np.ma.filled(masked_weight, fill_value=-np.inf)\n",
    "wei = softmax(filled_weight) # NOTE: WHY 10???\n",
    "print(wei)\n",
    "sol_3 = wei @ x\n",
    "# assert (sol_1 == sol_2).all()\n",
    "sol_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcb8ba9-097b-46f5-82ad-993e9bef6403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# third implementation\n",
    "# softmax = lambda x: np.exp(x) / np.sum(np.exp(x))\n",
    "k = x #(T, C)\n",
    "q = x\n",
    "v = x\n",
    "d_n = 16\n",
    "attn = q @ np.transpose(k, (0, 2, 1))  # (T, T)\n",
    "# attn /= 4\n",
    "attn_e = np.exp(attn)\n",
    "attn_sum = np.sum(attn_e)\n",
    "softmax = attn_e / attn_sum # using this instead of the above because there is no backward pass\n",
    "print(softmax.shape)\n",
    "attn /= attn_sum\n",
    "print(x.shape, attn.shape)\n",
    "sol_3 = attn @ v # (T, C)\n",
    "print(sol_3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e40811-2b5c-4a9b-88f8-e1ac486c2939",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(1, 8, 8).reshape(4, 2, 1)\n",
    "x.shape, x.T.shape, np.transpose(x, (0, 2, 1)).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
