{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "733c621c-24fc-4fba-8fe8-6287e5bac83c",
   "metadata": {},
   "source": [
    "Trying to understand scaling laws by [chinchilla]() using [nanogpt (scaling_laws.ipynb)]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c053b265-6472-49fe-be82-fbd37c431168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import functional as F\n",
    "import pandas as pd\n",
    "import scipy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79a7084f-9244-4d1d-964b-46518ef0d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_params(seq_len, vocab_size, d_model, num_heads, num_layers):\n",
    "    \"\"\" Given a GPT calculate total number of parameters \"\"\"\n",
    "    ffw_size = 4*d_model # Represent the intermediate layer size in MLP. in GPT the number of intermediate features is always 4*d_model.\n",
    "    # token and position embeddings\n",
    "    embeddings = d_model * vocab_size + d_model * seq_len\n",
    "    # transformer blocks\n",
    "    attention = 3*d_model**2 + 3*d_model # weights and biases\n",
    "    attproj = d_model**2 + d_model\n",
    "    ffw = d_model*(ffw_size) + ffw_size\n",
    "    ffwproj = ffw_size*d_model + d_model\n",
    "    layernorms = 2*2*d_model\n",
    "    # dense\n",
    "    ln_f = 2*d_model\n",
    "    dense = d_model*vocab_size # note: no bias here\n",
    "    # note: embeddings are not included in the param count!\n",
    "    total_params = num_layers*(attention + attproj + ffw + ffwproj + layernorms) + ln_f + dense\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2876f158-b602-4972-aafd-7b03a1c6eb10",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "\n",
    "A model's parameter size can simply be derived by running `.parameters()` to recieve the parameter count. However, intuitively let's understand, how the GPT parameters is calculated.\n",
    "\n",
    "`embedding = d_model * vocab_size + d_model * seq_len`\n",
    "embedding represents the total number of parameters used in the embedding layer. The embedding layer is made of two parts _token embedding_ and the _positional embedding_. The size of the weights used in the token embedding is `(number of input channels (d_model), vocab_size)`\n",
    "\n",
    "since \n",
    "```\n",
    "embedding layer    =  Train Data     @ Transpose(Weight)\n",
    "(B, T, vocab_size) = (B, vocab_size) @ (T, vocab_size)  \n",
    "```\n",
    "\n",
    "As you can see, after the training data is hot encoded it has a size of `B x vocab_size`. So based on this the Weight dimensions is `Size(vocab_size, T)`. where `T` represents the number of channels (aka. embedding output dimension). So the total number of parameters in the weight is `vocab_size x T`.\n",
    "\n",
    "Similarly in the positional embedding, the weight dimension is dictated by the training data.\n",
    "\n",
    "```\n",
    "embedding layer    =  Train Data  @ Transpose(Weight)\n",
    "(B, T, vocab_size) = (B, seq_len) @ (T, seq_len)  \n",
    "```\n",
    "\n",
    "Positional embedding is simply done by numbering the input within a fixed context window. This context window size is the `seq_len`(or sequence length).\n",
    "Based on this, the weight that embeds this data is of the `Size(seq_len, T)`. So making the total embedding of the positional embedding `seq_len x T`.\n",
    "\n",
    "The total number of parameters used in the embedding later is the sum of these two layers since the values are summed before passing through the next layer.\n",
    "\n",
    "```\n",
    "attention = 3*d_model**2 + 3*d_model\n",
    "```\n",
    "\n",
    "You've come across __Quadratic Scaling__ or __Squared attention__. Its the thought that when double the length of the input sequence, the computational cost associated with the attention mechanism increases by a factor of four because computational cost of attention in a transformer model scales quadratically with the sequence length.\n",
    "\n",
    "How does attention work?\n",
    "```python\n",
    "k = []\n",
    "for i in keys:\n",
    "    ki = []\n",
    "    for j in values:\n",
    "        for k in j:\n",
    "            ki[k] += j\n",
    "\n",
    "\n",
    "def attention(query, keys, values):\n",
    "    # Q, K, V  ->  Size(T, 3*C)\n",
    "    # Initialize attention weights\n",
    "    attention_weights = [] # (T, T)\n",
    "    \n",
    "    # Calculate attention scores\n",
    "    for key in keys:\n",
    "        score = dot_product(query, key) # (T, T)\n",
    "        attention_weights.append(score)\n",
    "    \n",
    "    # Normalize attention weights using softmax\n",
    "    attention_weights = softmax(attention_weights) # (T, T)\n",
    "    \n",
    "    # Calculate weighted sum of values\n",
    "    context_vector = [0] * len(values[0]) #(C,)\n",
    "    for i in range(len(values)):\n",
    "        for j in range(len(values[0])):\n",
    "            context_vector[j] += attention_weights[i] * values[i][j]\n",
    "    \n",
    "    return context_vector\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73c1d26f-f77e-4a80-b6fe-46c21ba360bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, C = 2, 4, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fb64b530-3d2d-4c13-8a3a-ba4e6c096430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.,  2.],\n",
       "        [ 2.,  3.],\n",
       "        [ 3.,  4.],\n",
       "        [ 4.,  5.]],\n",
       "\n",
       "       [[ 9., 10.],\n",
       "        [10., 11.],\n",
       "        [11., 12.],\n",
       "        [12., 13.]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first implementation\n",
    "np.random.seed(10)\n",
    "x = np.linspace(1, 16, 16).reshape(B, T, C) \n",
    "# x /= B*T*C\n",
    "sol_1 = np.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        x_prev = x[b, :t+1] #(t, C)\n",
    "        sol_1[b, t] = np.mean(x_prev, 0)\n",
    "        \n",
    "\n",
    "sol_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ac652bcd-c161-4859-9d59-8df67f5c31a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.         0.         0.         0.        ]\n",
      "  [0.5        0.5        0.         0.        ]\n",
      "  [0.33333333 0.33333333 0.33333333 0.        ]\n",
      "  [0.25       0.25       0.25       0.25      ]]\n",
      "\n",
      " [[1.         0.         0.         0.        ]\n",
      "  [0.5        0.5        0.         0.        ]\n",
      "  [0.33333333 0.33333333 0.33333333 0.        ]\n",
      "  [0.25       0.25       0.25       0.25      ]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 1.,  2.],\n",
       "        [ 2.,  3.],\n",
       "        [ 3.,  4.],\n",
       "        [ 4.,  5.]],\n",
       "\n",
       "       [[ 9., 10.],\n",
       "        [10., 11.],\n",
       "        [11., 12.],\n",
       "        [12., 13.]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second implementation\n",
    "\n",
    "wei = np.tril(np.ones((B, T, T)))\n",
    "wei = wei /wei.sum(2, keepdims=True)\n",
    "sol_2 = wei @ x\n",
    "print(wei)\n",
    "assert (sol_1 == sol_2).all()\n",
    "sol_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1fbbb63d-86e2-44bd-a55e-5b1d9f3fa860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.         0.         0.         0.        ]\n",
      "  [0.5        0.5        0.         0.        ]\n",
      "  [0.33333333 0.33333333 0.33333333 0.        ]\n",
      "  [0.25       0.25       0.25       0.25      ]]\n",
      "\n",
      " [[1.         0.         0.         0.        ]\n",
      "  [0.5        0.5        0.         0.        ]\n",
      "  [0.33333333 0.33333333 0.33333333 0.        ]\n",
      "  [0.25       0.25       0.25       0.25      ]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 1.,  2.],\n",
       "        [ 2.,  3.],\n",
       "        [ 3.,  4.],\n",
       "        [ 4.,  5.]],\n",
       "\n",
       "       [[ 9., 10.],\n",
       "        [10., 11.],\n",
       "        [11., 12.],\n",
       "        [12., 13.]]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# third implementation\n",
    "softmax = lambda v: np.exp(v) / np.exp(v).sum(2, keepdims=True) # hmm, only finding the softmax along the T dimension works. So batch should be ignored??\n",
    "\n",
    "c = np.tril(np.ones((B, T, T)))\n",
    "sol_3 = np.zeros((B, T, T))\n",
    "masked_weight = np.ma.masked_array(sol_3, mask=(c==0))\n",
    "filled_weight = np.ma.filled(masked_weight, fill_value=-np.inf) # fill all zeros to with -Inf\n",
    "wei = softmax(filled_weight) # NOTE: WHY 10???\n",
    "print(wei)\n",
    "sol_3 = wei @ x\n",
    "\n",
    "assert (sol_1 == sol_3).all()\n",
    "sol_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dfcb8ba9-097b-46f5-82ad-993e9bef6403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.         0.         0.         0.        ]\n",
      "  [0.5        0.5        0.         0.        ]\n",
      "  [0.33333333 0.33333333 0.33333333 0.        ]\n",
      "  [0.25       0.25       0.25       0.25      ]]\n",
      "\n",
      " [[1.         0.         0.         0.        ]\n",
      "  [0.5        0.5        0.         0.        ]\n",
      "  [0.33333333 0.33333333 0.33333333 0.        ]\n",
      "  [0.25       0.25       0.25       0.25      ]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 1.,  2.],\n",
       "        [ 2.,  3.],\n",
       "        [ 3.,  4.],\n",
       "        [ 4.,  5.]],\n",
       "\n",
       "       [[ 9., 10.],\n",
       "        [10., 11.],\n",
       "        [11., 12.],\n",
       "        [12., 13.]]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forth implementation\n",
    "\n",
    "softmax = lambda v: np.exp(v) / np.exp(v).sum(2, keepdims=True) # hmm, only finding the softmax along the T dimension works. So batch should be ignored??\n",
    "k = np.ones((B, T, C))\n",
    "q = np.ones((B, T, C)) \n",
    "v = x #np.ones((B, T, C))\n",
    "\n",
    "d_n = 16\n",
    "attn = q @ np.transpose(k, (0, 2, 1)) / C # (T, T)\n",
    "\n",
    "c = np.tril(np.ones((B, T, T)))\n",
    "masked_weight = np.ma.masked_array(attn, mask=(c==0))\n",
    "filled_weight = np.ma.filled(masked_weight, fill_value=-np.inf) # fill all zeros to with -Inf\n",
    "wei = softmax(filled_weight) # NOTE: WHY 10???\n",
    "print(wei)\n",
    "sol_4 = wei @ v\n",
    "\n",
    "assert (sol_1 == sol_4).all()\n",
    "sol_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6fc8a3-2ee1-43e9-9759-da67eeae8c95",
   "metadata": {},
   "source": [
    "ohh!! Initially thought Q, K, V was inspired by a theory with a similar trifecta. However that doesn't seem to be the case. \n",
    "It seems as though, it was inspired by information retrival or something of that sought but nothing pertaining numerical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "63e40811-2b5c-4a9b-88f8-e1ac486c2939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 3.],\n",
       "       [7., 7.]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(1, 4, 4).reshape(2, 2) @ np.ones((2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6927dcae-1a35-400e-8398-615dc70dee61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.         0.         0.         0.        ]\n",
      "  [0.5        0.5        0.         0.        ]\n",
      "  [0.33333333 0.33333333 0.33333333 0.        ]\n",
      "  [0.25       0.25       0.25       0.25      ]]\n",
      "\n",
      " [[1.         0.         0.         0.        ]\n",
      "  [0.5        0.5        0.         0.        ]\n",
      "  [0.33333333 0.33333333 0.33333333 0.        ]\n",
      "  [0.25       0.25       0.25       0.25      ]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 1.5,  1.5,  1.5,  1.5,  1.5,  1.5],\n",
       "        [ 2.5,  2.5,  2.5,  2.5,  2.5,  2.5],\n",
       "        [ 3.5,  3.5,  3.5,  3.5,  3.5,  3.5],\n",
       "        [ 4.5,  4.5,  4.5,  4.5,  4.5,  4.5]],\n",
       "\n",
       "       [[ 9.5,  9.5,  9.5,  9.5,  9.5,  9.5],\n",
       "        [10.5, 10.5, 10.5, 10.5, 10.5, 10.5],\n",
       "        [11.5, 11.5, 11.5, 11.5, 11.5, 11.5],\n",
       "        [12.5, 12.5, 12.5, 12.5, 12.5, 12.5]]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fifth implementation\n",
    "query = lambda v: v @ (np.ones((B, C, 3*C)) / C)\n",
    "key = lambda v: v @ (np.ones((B, C, 3*C)) / C)\n",
    "value = lambda v: v @ (np.ones((B, C, 3*C)) / C)\n",
    "\n",
    "softmax = lambda v: np.exp(v) / np.exp(v).sum(2, keepdims=True) # hmm, only finding the softmax along the T dimension works. So batch should be ignored??\n",
    "k = np.ones((B, T, C))\n",
    "q = np.ones((B, T, C)) \n",
    "v = x #np.ones((B, T, C))\n",
    "# print(key(k)[0], query(q)[0], value(v)[0])\n",
    "d_n = 16\n",
    "attn = (query(q) @ np.transpose(key(k), (0, 2, 1))) / (3*C) # (T, T)\n",
    "\n",
    "# print(attn, attn.shape)\n",
    "c = np.tril(np.ones((B, T, T)))\n",
    "masked_weight = np.ma.masked_array(attn, mask=(c==0))\n",
    "filled_weight = np.ma.filled(masked_weight, fill_value=-np.inf) # fill all zeros to with -Inf\n",
    "wei = softmax(filled_weight) # NOTE: WHY 10???\n",
    "print(wei)\n",
    "sol_5 = wei @ value(v)\n",
    "\n",
    "assert np.allclose(query(sol_1), sol_5)\n",
    "sol_5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
